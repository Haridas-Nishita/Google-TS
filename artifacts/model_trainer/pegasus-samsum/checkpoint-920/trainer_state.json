{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.9991854466467553,
  "eval_steps": 500,
  "global_step": 920,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.010860711376595167,
      "grad_norm": 216.87985229492188,
      "learning_rate": 5.000000000000001e-07,
      "loss": 51.5251,
      "step": 10
    },
    {
      "epoch": 0.021721422753190334,
      "grad_norm": 294.5408630371094,
      "learning_rate": 1.4000000000000001e-06,
      "loss": 48.1969,
      "step": 20
    },
    {
      "epoch": 0.0325821341297855,
      "grad_norm": 201.54391479492188,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 50.3051,
      "step": 30
    },
    {
      "epoch": 0.04344284550638067,
      "grad_norm": 185.25440979003906,
      "learning_rate": 3.4000000000000005e-06,
      "loss": 49.0289,
      "step": 40
    },
    {
      "epoch": 0.054303556882975834,
      "grad_norm": 176.5139923095703,
      "learning_rate": 4.4e-06,
      "loss": 43.6578,
      "step": 50
    },
    {
      "epoch": 0.065164268259571,
      "grad_norm": 796.4702758789062,
      "learning_rate": 5.4e-06,
      "loss": 44.0485,
      "step": 60
    },
    {
      "epoch": 0.07602497963616617,
      "grad_norm": 145.081298828125,
      "learning_rate": 6.2e-06,
      "loss": 44.9089,
      "step": 70
    },
    {
      "epoch": 0.08688569101276133,
      "grad_norm": 160.7222137451172,
      "learning_rate": 7.2e-06,
      "loss": 41.7967,
      "step": 80
    },
    {
      "epoch": 0.0977464023893565,
      "grad_norm": 132.01901245117188,
      "learning_rate": 8.200000000000001e-06,
      "loss": 39.0197,
      "step": 90
    },
    {
      "epoch": 0.10860711376595167,
      "grad_norm": 184.6796875,
      "learning_rate": 9.2e-06,
      "loss": 42.102,
      "step": 100
    },
    {
      "epoch": 0.11946782514254684,
      "grad_norm": 126.36277770996094,
      "learning_rate": 1.02e-05,
      "loss": 38.1009,
      "step": 110
    },
    {
      "epoch": 0.130328536519142,
      "grad_norm": 347.0696105957031,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 36.8008,
      "step": 120
    },
    {
      "epoch": 0.14118924789573717,
      "grad_norm": 105.01244354248047,
      "learning_rate": 1.22e-05,
      "loss": 34.241,
      "step": 130
    },
    {
      "epoch": 0.15204995927233234,
      "grad_norm": 123.0274658203125,
      "learning_rate": 1.32e-05,
      "loss": 35.5097,
      "step": 140
    },
    {
      "epoch": 0.1629106706489275,
      "grad_norm": 119.07234954833984,
      "learning_rate": 1.42e-05,
      "loss": 35.8649,
      "step": 150
    },
    {
      "epoch": 0.17377138202552267,
      "grad_norm": 106.0689926147461,
      "learning_rate": 1.52e-05,
      "loss": 32.1291,
      "step": 160
    },
    {
      "epoch": 0.18463209340211784,
      "grad_norm": 89.52690887451172,
      "learning_rate": 1.62e-05,
      "loss": 32.7404,
      "step": 170
    },
    {
      "epoch": 0.195492804778713,
      "grad_norm": 113.12354278564453,
      "learning_rate": 1.7199999999999998e-05,
      "loss": 31.872,
      "step": 180
    },
    {
      "epoch": 0.20635351615530817,
      "grad_norm": 176.35635375976562,
      "learning_rate": 1.8200000000000002e-05,
      "loss": 29.7881,
      "step": 190
    },
    {
      "epoch": 0.21721422753190334,
      "grad_norm": 184.48411560058594,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 31.3831,
      "step": 200
    },
    {
      "epoch": 0.2280749389084985,
      "grad_norm": 108.31778717041016,
      "learning_rate": 2.0200000000000003e-05,
      "loss": 30.8848,
      "step": 210
    },
    {
      "epoch": 0.23893565028509367,
      "grad_norm": 76.5313491821289,
      "learning_rate": 2.12e-05,
      "loss": 28.6254,
      "step": 220
    },
    {
      "epoch": 0.24979636166168884,
      "grad_norm": 104.5726547241211,
      "learning_rate": 2.22e-05,
      "loss": 29.81,
      "step": 230
    },
    {
      "epoch": 0.260657073038284,
      "grad_norm": 65.67223358154297,
      "learning_rate": 2.32e-05,
      "loss": 28.5103,
      "step": 240
    },
    {
      "epoch": 0.27151778441487917,
      "grad_norm": 62.38520812988281,
      "learning_rate": 2.4200000000000002e-05,
      "loss": 29.9277,
      "step": 250
    },
    {
      "epoch": 0.28237849579147434,
      "grad_norm": 74.98091888427734,
      "learning_rate": 2.5200000000000003e-05,
      "loss": 29.0945,
      "step": 260
    },
    {
      "epoch": 0.2932392071680695,
      "grad_norm": 105.14060974121094,
      "learning_rate": 2.6200000000000003e-05,
      "loss": 28.587,
      "step": 270
    },
    {
      "epoch": 0.30409991854466467,
      "grad_norm": 74.68124389648438,
      "learning_rate": 2.7200000000000004e-05,
      "loss": 28.0324,
      "step": 280
    },
    {
      "epoch": 0.31496062992125984,
      "grad_norm": 70.51219940185547,
      "learning_rate": 2.8199999999999998e-05,
      "loss": 29.5457,
      "step": 290
    },
    {
      "epoch": 0.325821341297855,
      "grad_norm": 66.75056457519531,
      "learning_rate": 2.9199999999999998e-05,
      "loss": 27.1656,
      "step": 300
    },
    {
      "epoch": 0.33668205267445017,
      "grad_norm": 71.70661163330078,
      "learning_rate": 3.02e-05,
      "loss": 29.819,
      "step": 310
    },
    {
      "epoch": 0.34754276405104534,
      "grad_norm": 357.1569519042969,
      "learning_rate": 3.12e-05,
      "loss": 30.4634,
      "step": 320
    },
    {
      "epoch": 0.3584034754276405,
      "grad_norm": 95.5864486694336,
      "learning_rate": 3.2200000000000003e-05,
      "loss": 29.3532,
      "step": 330
    },
    {
      "epoch": 0.3692641868042357,
      "grad_norm": 62.94727325439453,
      "learning_rate": 3.32e-05,
      "loss": 27.5707,
      "step": 340
    },
    {
      "epoch": 0.38012489818083084,
      "grad_norm": 62.82508087158203,
      "learning_rate": 3.4200000000000005e-05,
      "loss": 27.5926,
      "step": 350
    },
    {
      "epoch": 0.390985609557426,
      "grad_norm": 65.9097900390625,
      "learning_rate": 3.52e-05,
      "loss": 26.357,
      "step": 360
    },
    {
      "epoch": 0.4018463209340212,
      "grad_norm": 67.52824401855469,
      "learning_rate": 3.62e-05,
      "loss": 26.888,
      "step": 370
    },
    {
      "epoch": 0.41270703231061634,
      "grad_norm": 89.29817962646484,
      "learning_rate": 3.72e-05,
      "loss": 27.4377,
      "step": 380
    },
    {
      "epoch": 0.4235677436872115,
      "grad_norm": 61.82282257080078,
      "learning_rate": 3.82e-05,
      "loss": 26.9941,
      "step": 390
    },
    {
      "epoch": 0.4344284550638067,
      "grad_norm": 93.0180892944336,
      "learning_rate": 3.9200000000000004e-05,
      "loss": 27.4411,
      "step": 400
    },
    {
      "epoch": 0.44528916644040184,
      "grad_norm": 79.62981414794922,
      "learning_rate": 4.02e-05,
      "loss": 27.5303,
      "step": 410
    },
    {
      "epoch": 0.456149877816997,
      "grad_norm": 214.61355590820312,
      "learning_rate": 4.12e-05,
      "loss": 25.9448,
      "step": 420
    },
    {
      "epoch": 0.4670105891935922,
      "grad_norm": 87.1008529663086,
      "learning_rate": 4.22e-05,
      "loss": 28.9981,
      "step": 430
    },
    {
      "epoch": 0.47787130057018734,
      "grad_norm": 123.88618469238281,
      "learning_rate": 4.32e-05,
      "loss": 27.3618,
      "step": 440
    },
    {
      "epoch": 0.4887320119467825,
      "grad_norm": 91.87103271484375,
      "learning_rate": 4.4200000000000004e-05,
      "loss": 26.4004,
      "step": 450
    },
    {
      "epoch": 0.4995927233233777,
      "grad_norm": 78.36914825439453,
      "learning_rate": 4.52e-05,
      "loss": 27.2731,
      "step": 460
    },
    {
      "epoch": 0.5104534346999728,
      "grad_norm": 63.67753601074219,
      "learning_rate": 4.6200000000000005e-05,
      "loss": 27.1015,
      "step": 470
    },
    {
      "epoch": 0.521314146076568,
      "grad_norm": 225.7515411376953,
      "learning_rate": 4.72e-05,
      "loss": 26.2948,
      "step": 480
    },
    {
      "epoch": 0.5321748574531632,
      "grad_norm": 82.21100616455078,
      "learning_rate": 4.82e-05,
      "loss": 26.3399,
      "step": 490
    },
    {
      "epoch": 0.5430355688297583,
      "grad_norm": 64.64801788330078,
      "learning_rate": 4.92e-05,
      "loss": 26.4545,
      "step": 500
    },
    {
      "epoch": 0.5430355688297583,
      "eval_loss": 1.485790729522705,
      "eval_runtime": 46.0107,
      "eval_samples_per_second": 17.778,
      "eval_steps_per_second": 17.778,
      "step": 500
    },
    {
      "epoch": 0.5538962802063535,
      "grad_norm": 59.182518005371094,
      "learning_rate": 4.976190476190477e-05,
      "loss": 26.6872,
      "step": 510
    },
    {
      "epoch": 0.5647569915829487,
      "grad_norm": 60.61927032470703,
      "learning_rate": 4.8571428571428576e-05,
      "loss": 26.1539,
      "step": 520
    },
    {
      "epoch": 0.5756177029595438,
      "grad_norm": 60.32405471801758,
      "learning_rate": 4.738095238095238e-05,
      "loss": 27.0493,
      "step": 530
    },
    {
      "epoch": 0.586478414336139,
      "grad_norm": 72.5730972290039,
      "learning_rate": 4.6190476190476194e-05,
      "loss": 25.0343,
      "step": 540
    },
    {
      "epoch": 0.5973391257127342,
      "grad_norm": 153.36769104003906,
      "learning_rate": 4.5e-05,
      "loss": 26.809,
      "step": 550
    },
    {
      "epoch": 0.6081998370893293,
      "grad_norm": 78.21500396728516,
      "learning_rate": 4.380952380952381e-05,
      "loss": 27.0914,
      "step": 560
    },
    {
      "epoch": 0.6190605484659245,
      "grad_norm": 92.45213317871094,
      "learning_rate": 4.261904761904762e-05,
      "loss": 27.3623,
      "step": 570
    },
    {
      "epoch": 0.6299212598425197,
      "grad_norm": 210.00082397460938,
      "learning_rate": 4.1547619047619054e-05,
      "loss": 26.2803,
      "step": 580
    },
    {
      "epoch": 0.6407819712191148,
      "grad_norm": 52.807071685791016,
      "learning_rate": 4.035714285714286e-05,
      "loss": 24.9116,
      "step": 590
    },
    {
      "epoch": 0.65164268259571,
      "grad_norm": 60.23068618774414,
      "learning_rate": 3.9166666666666665e-05,
      "loss": 26.8566,
      "step": 600
    },
    {
      "epoch": 0.6625033939723052,
      "grad_norm": 115.24188995361328,
      "learning_rate": 3.7976190476190474e-05,
      "loss": 25.5744,
      "step": 610
    },
    {
      "epoch": 0.6733641053489003,
      "grad_norm": 58.34044647216797,
      "learning_rate": 3.678571428571429e-05,
      "loss": 26.2675,
      "step": 620
    },
    {
      "epoch": 0.6842248167254955,
      "grad_norm": 114.89396667480469,
      "learning_rate": 3.55952380952381e-05,
      "loss": 26.4387,
      "step": 630
    },
    {
      "epoch": 0.6950855281020907,
      "grad_norm": 62.193599700927734,
      "learning_rate": 3.440476190476191e-05,
      "loss": 26.2419,
      "step": 640
    },
    {
      "epoch": 0.7059462394786858,
      "grad_norm": 82.61676025390625,
      "learning_rate": 3.321428571428572e-05,
      "loss": 24.8149,
      "step": 650
    },
    {
      "epoch": 0.716806950855281,
      "grad_norm": 51.1771240234375,
      "learning_rate": 3.202380952380952e-05,
      "loss": 24.9714,
      "step": 660
    },
    {
      "epoch": 0.7276676622318762,
      "grad_norm": 52.771461486816406,
      "learning_rate": 3.0833333333333335e-05,
      "loss": 25.6587,
      "step": 670
    },
    {
      "epoch": 0.7385283736084713,
      "grad_norm": 461.33306884765625,
      "learning_rate": 2.9642857142857144e-05,
      "loss": 25.5046,
      "step": 680
    },
    {
      "epoch": 0.7493890849850665,
      "grad_norm": 62.68887710571289,
      "learning_rate": 2.8452380952380953e-05,
      "loss": 24.9459,
      "step": 690
    },
    {
      "epoch": 0.7602497963616617,
      "grad_norm": 61.05050277709961,
      "learning_rate": 2.7261904761904762e-05,
      "loss": 26.3117,
      "step": 700
    },
    {
      "epoch": 0.7711105077382568,
      "grad_norm": 65.5767822265625,
      "learning_rate": 2.6071428571428574e-05,
      "loss": 25.3835,
      "step": 710
    },
    {
      "epoch": 0.781971219114852,
      "grad_norm": 48.51728439331055,
      "learning_rate": 2.4880952380952383e-05,
      "loss": 24.9976,
      "step": 720
    },
    {
      "epoch": 0.7928319304914472,
      "grad_norm": 60.70817947387695,
      "learning_rate": 2.369047619047619e-05,
      "loss": 24.9385,
      "step": 730
    },
    {
      "epoch": 0.8036926418680423,
      "grad_norm": 49.32307815551758,
      "learning_rate": 2.25e-05,
      "loss": 26.7446,
      "step": 740
    },
    {
      "epoch": 0.8145533532446375,
      "grad_norm": 89.40241241455078,
      "learning_rate": 2.130952380952381e-05,
      "loss": 24.9905,
      "step": 750
    },
    {
      "epoch": 0.8254140646212327,
      "grad_norm": 73.81769561767578,
      "learning_rate": 2.011904761904762e-05,
      "loss": 25.5499,
      "step": 760
    },
    {
      "epoch": 0.8362747759978278,
      "grad_norm": 120.85413360595703,
      "learning_rate": 1.892857142857143e-05,
      "loss": 24.9742,
      "step": 770
    },
    {
      "epoch": 0.847135487374423,
      "grad_norm": 73.66728973388672,
      "learning_rate": 1.773809523809524e-05,
      "loss": 24.9753,
      "step": 780
    },
    {
      "epoch": 0.8579961987510182,
      "grad_norm": 77.41060638427734,
      "learning_rate": 1.6547619047619046e-05,
      "loss": 24.6906,
      "step": 790
    },
    {
      "epoch": 0.8688569101276133,
      "grad_norm": 67.93286895751953,
      "learning_rate": 1.535714285714286e-05,
      "loss": 26.4626,
      "step": 800
    },
    {
      "epoch": 0.8797176215042085,
      "grad_norm": 48.78627395629883,
      "learning_rate": 1.4166666666666668e-05,
      "loss": 24.7464,
      "step": 810
    },
    {
      "epoch": 0.8905783328808037,
      "grad_norm": 128.1563720703125,
      "learning_rate": 1.2976190476190478e-05,
      "loss": 25.9228,
      "step": 820
    },
    {
      "epoch": 0.9014390442573988,
      "grad_norm": 59.67975616455078,
      "learning_rate": 1.1785714285714286e-05,
      "loss": 25.9587,
      "step": 830
    },
    {
      "epoch": 0.912299755633994,
      "grad_norm": 55.814552307128906,
      "learning_rate": 1.0595238095238096e-05,
      "loss": 24.3387,
      "step": 840
    },
    {
      "epoch": 0.9231604670105892,
      "grad_norm": 64.34091186523438,
      "learning_rate": 9.404761904761905e-06,
      "loss": 24.6612,
      "step": 850
    },
    {
      "epoch": 0.9340211783871843,
      "grad_norm": 61.19854736328125,
      "learning_rate": 8.214285714285714e-06,
      "loss": 25.313,
      "step": 860
    },
    {
      "epoch": 0.9448818897637795,
      "grad_norm": 68.56942749023438,
      "learning_rate": 7.023809523809524e-06,
      "loss": 25.5715,
      "step": 870
    },
    {
      "epoch": 0.9557426011403747,
      "grad_norm": 51.376834869384766,
      "learning_rate": 5.833333333333334e-06,
      "loss": 24.6312,
      "step": 880
    },
    {
      "epoch": 0.9666033125169698,
      "grad_norm": 53.674560546875,
      "learning_rate": 4.642857142857143e-06,
      "loss": 25.2737,
      "step": 890
    },
    {
      "epoch": 0.977464023893565,
      "grad_norm": 68.48633575439453,
      "learning_rate": 3.4523809523809528e-06,
      "loss": 25.0103,
      "step": 900
    },
    {
      "epoch": 0.9883247352701602,
      "grad_norm": 74.07582092285156,
      "learning_rate": 2.2619047619047617e-06,
      "loss": 24.4576,
      "step": 910
    },
    {
      "epoch": 0.9991854466467553,
      "grad_norm": 66.78843688964844,
      "learning_rate": 1.0714285714285716e-06,
      "loss": 25.5025,
      "step": 920
    }
  ],
  "logging_steps": 10,
  "max_steps": 920,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 1000000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 5528248038285312.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
